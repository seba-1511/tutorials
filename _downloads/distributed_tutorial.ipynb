{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSyntaxError\n===========\n\nExample script with invalid Python syntax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n\"\"\"\"\nWriting Distributed Applications with PyTorch\n**********************************************\n**Author**: `Seb Arnold <https://seba1511.com>`_\n\nIntroduction\n============\n\nIn this short tutorial, we will be going over the distributed package of\nPyTorch. We'll see how to set up the distributed setting, use the\ndifferent communication strategies, and go over some the internals of\nthe package.\n\nSetup\n=====\n\n.. raw:: html\n\n   <!--\n   * Processes & machines\n   * variables and init_process_group\n   -->\n\nThe distributed package included ting Distributed Applications with\nPyTorch in PyTorch (i.e., ``torch.distributed``) enables researchers and\npractitioners to easily parallelize their computations across processes\nand clusters of machines. To do so, it leverages the messaging passing\nsemantics allowing each process to communicate data to any of the other\nprocesses. As opposed to the multiprocessing (``torch.multiprocessing``)\npackage, processes can use different communication backends and are not\nrestricted to being executed on the same machine.\n\nIn order to get started we need the ability to run multiple processes\nsimultaneously. If you have access to compute cluster you should check\nwith your local sysadmin or use your favorite coordination tool. (e.g.,\n`pdsh <https://linux.die.net/man/1/pdsh>`__,\n`clustershell <http://cea-hpc.github.io/clustershell/>`__, or\n`others <https://slurm.schedmd.com/>`__) For the purpose of this\ntutorial, we will use a single machine and fork multiple processes using\nthe following template.\n\n.. code:: python\n\n    \"\"\"run.py:\"\"\"\n    #!/usr/bin/env python\n    import os\n    import torch\n    import torch.distributed as dist\n    from torch.multiprocessing import Process\n\n    def run(rank, size):\n        \"\"\" Distributed function to be implemented later. \"\"\"\n        pass\n\n    def init_processes(rank, size, fn, backend='tcp'):\n        \"\"\" Initialize the distributed environment. \"\"\"\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = '29500'\n        dist.init_process_group(backend, rank=rank, world_size=size)\n        fn(rank, size)\n\n\n    if __name__ == \"__main__\":\n        size = 2\n        processes = []\n        for rank in range(size):\n            p = Process(target=init_processes, args=(rank, size, run))\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\nThe above script spawns two processes who will each setup the\ndistributed environment, initialize the process group\n(``dist.init_process_group``), and finally execute the given ``run``\nfunction.\n\nLet's have a look at the ``init_processes`` function. It ensures that\nevery process will be able to coordinate through a master, using the\nsame ip address and port. Note that we used the TCP backend, but we\ncould have used\n`MPI <https://en.wikipedia.org/wiki/Message_Passing_Interface>`__ or\n`Gloo <http://github.com/facebookincubator/gloo>`__ instead. (c.f.\n`Section 5.1 <#communication-backends>`__) We will go over the magic\nhappening in ``dist.init_process_group`` at the end of this tutorial,\nbut it essentially allows processes to communicate with each other by\nsharing their locations.\n\nPoint-to-Point Communication\n============================\n\n.. raw:: html\n\n   <!--\n   * send/recv\n   * isend/irecv\n   -->\n\n.. raw:: html\n\n   <table>\n\n.. raw:: html\n\n   <tbody>\n\n.. raw:: html\n\n   <tr>\n\n.. raw:: html\n\n   </tr>\n\n.. raw:: html\n\n   <tr>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n Send and Recv\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   </tr>\n\n.. raw:: html\n\n   </tbody>\n\n.. raw:: html\n\n   </table>\n\nA transfer of data from one process to another is called a\npoint-to-point communication. These are achieved through the ``send``\nand ``recv`` functions or their *immediate* counter-parts, ``isend`` and\n``irecv``.\n\n.. code:: python\n\n    \"\"\"Blocking point-to-point communication.\"\"\"\n\n    def run(rank, size):\n        tensor = torch.zeros(1)\n        if rank == 0:\n            tensor += 1\n            # Send the tensor to process 1\n            dist.send(tensor=tensor, dst=1)\n        else:\n            # Receive tensor from process 0\n            dist.recv(tensor=tensor, src=0)\n        print('Rank ', rank, ' has data ', tensor[0])\n\nIn the above example, both processes start with a zero tensor, then\nprocess 0 increments the tensor and sends it to process 1 so that they\nboth end up with 1.0. Notice that process 1 needs to allocate memory in\norder to store the data it will receive.\n\nAlso notice that ``send``/``recv`` are **blocking**: both processes stop\nuntil the communication is completed. On the other hand immediates are\n**non-blocking**; the script continues its execution and the methods\nreturn a ``DistributedRequest`` object upon which we can choose to\n``wait()``.\n\n.. code:: python\n\n    \"\"\"Non-blocking point-to-point communication.\"\"\"\n\n    def run(rank, size):\n        tensor = torch.zeros(1)\n        req = None\n        if rank == 0:\n            tensor += 1\n            # Send the tensor to process 1\n            req = dist.isend(tensor=tensor, dst=1)\n            print('Rank 0 started sending')\n        else:\n            # Receive tensor from process 0\n            req = dist.irecv(tensor=tensor, src=0)\n            print('Rank 1 started receiving')\n            print('Rank 1 has data ', tensor[0])\n        req.wait()\n        print('Rank ', rank, ' has data ', tensor[0])\n\nRunning the above function might result in process 1 still having 0.0\nwhile having already started receiving. However, after ``req.wait()``\nhas been executed we are guaranteed that the communication took place,\nand that the value stored in ``tensor[0]`` is 1.0.\n\nPoint-to-point communication is useful when we want a fine-grained\ncontrol over the communication of our processes. They can be used to\nimplement fancy algorithms, such as the one used in `Baidu's\nDeepSpeech <https://github.com/baidu-research/baidu-allreduce>`__ or\n`Facebook's large-scale\nexperiments <https://research.fb.com/publications/imagenet1kin1h/>`__.(c.f.\n`Section 4.1 <#our-own-ring-allreduce>`__)\n\nCollective Communication\n========================\n\n.. raw:: html\n\n   <!--\n   * gather\n   * reduce\n   * broadcast\n   * scatter\n   * all_reduce\n   -->\n\n.. raw:: html\n\n   <table>\n\n.. raw:: html\n\n   <tbody>\n\n.. raw:: html\n\n   <tr>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n Broadcast\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n AllGather\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   </tr>\n\n.. raw:: html\n\n   <tr>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n Reduce\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n AllReduce\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   </tr>\n\n.. raw:: html\n\n   <tr>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n Scatter\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   <td align=\"center\">\n\n Gather\n\n.. raw:: html\n\n   </td>\n\n.. raw:: html\n\n   </tr>\n\n.. raw:: html\n\n   </tbody>\n\n.. raw:: html\n\n   </table>\n\nAs opposed to point-to-point communcation, collectives allow for\ncommunication patterns across all processes in a **group**. A group is a\nsubset of all our processes. To create a group, we can pass a list of\nranks to ``dist.new_group(group)``. By default, collectives are executed\non the all processes, also known as the **world**. For example, in order\nto obtain the sum of all tensors at all processes, we can use the\n``dist.all_reduce(tensor, op, group)`` collective.\n\n.. code:: python\n\n    \"\"\" All-Reduce example.\"\"\"\n    def run(rank, size):\n        \"\"\" Simple point-to-point communication. \"\"\"\n        group = dist.new_group([0, 1]) \n        tensor = torch.ones(1)\n        dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n        print('Rank ', rank, ' has data ', tensor[0])\n\nSince we want the sum of all tensors in the group, we use\n``dist.reduce_op.SUM`` as the reduce operator. Generally speaking, any\ncommutative mathematical operation can be used as an operator.\nOut-of-the-box, PyTorch comes with 4 such operators, all working at the\nelement-wise level:\n\n-  ``dist.reduce_op.SUM``,\n-  ``dist.reduce_op.PRODUCT``,\n-  ``dist.reduce_op.MAX``,\n-  ``dist.reduce_op.MIN``.\n\nIn addition to ``dist.all_reduce(tensor, op, group)``, there are a total\nof 6 collectives currently implemented in PyTorch.\n\n-  ``dist.broadcast(tensor, src, group)``: Copies ``tensor`` from\n   ``src`` to all other processes.\n-  ``dist.reduce(tensor, dst, op, group)``: Applies ``op`` to all\n   ``tensor`` and stores the result in ``dst``.\n-  ``dist.all_reduce(tensor, op, group)``: Same as reduce, but the\n   result is stored in all processes.\n-  ``dist.scatter(tensor, src, scatter_list, group)``: Copies the\n   :math:`i^{\\text{th}}` tensor ``scatter_list[i]`` to the\n   :math:`i^{\\text{th}}` process.\n-  ``dist.gather(tensor, dst, gather_list, group)``: Copies ``tensor``\n   from all processes in ``dst``.\n-  ``dist.all_gather(tensor_list, tensor, group)``: Copies ``tensor``\n   from all processes to ``tensor_list``, on all processes.\n\nDistributed Training\n====================\n\n.. raw:: html\n\n   <!--\n   * Gloo Backend\n   * Simple all_reduce on the gradients\n   * Point to optimized DistributedDataParallel\n\n   TODO: Custom ring-allreduce\n   -->\n\n**Note:** You can find the example script of this section in `this\nGitHub repository <https://github.com/seba-1511/dist_tuto.pth/>`__.\n\nNow that we understand how the distributed module works, let us write\nsomething useful with it. Our goal will be to replicate the\nfunctionality of\n`DistributedDataParallel <http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel>`__.\nOf course, this will be a didactic example and in a real-world\nsitutation you should use the official, well-tested and well-optimized\nversion linked above.\n\nQuite simply we want to implement a distributed version of stochastic\ngradient descent. Our script will let all processes compute the\ngradients of their model on their batch of data and then average their\ngradients. In order to ensure similar convergence results when changing\nthe number of processes, we will first have to partition our dataset.\n(You could also use\n`tnt.dataset.SplitDataset <https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4>`__,\ninstead of the snippet below.)\n\n.. code:: python\n\n    \"\"\" Dataset partitioning helper \"\"\"\n    class Partition(object):\n\n        def __init__(self, data, index):\n            self.data = data\n            self.index = index\n\n        def __len__(self):\n            return len(self.index)\n\n        def __getitem__(self, index):\n            data_idx = self.index[index]\n            return self.data[data_idx]\n\n\n    class DataPartitioner(object):\n\n        def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n            self.data = data\n            self.partitions = []\n            rng = Random()\n            rng.seed(seed)\n            data_len = len(data)\n            indexes = [x for x in range(0, data_len)]\n            rng.shuffle(indexes)\n\n            for frac in sizes:\n                part_len = int(frac * data_len)\n                self.partitions.append(indexes[0:part_len])\n                indexes = indexes[part_len:]\n\n        def use(self, partition):\n            return Partition(self.data, self.partitions[partition])\n\nWith the above snippet, we can now simply partition any dataset using\nthe following few lines:\n\n.. code:: python\n\n    \"\"\" Partitioning MNIST \"\"\"\n    def partition_dataset():\n        dataset = datasets.MNIST('./data', train=True, download=True,\n                                 transform=transforms.Compose([\n                                     transforms.ToTensor(),\n                                     transforms.Normalize((0.1307,), (0.3081,))\n                                 ]))\n        size = dist.get_world_size()\n        bsz = 128 / float(size)\n        partition_sizes = [1.0 / size for _ in range(size)]\n        partition = DataPartitioner(dataset, partition_sizes)\n        partition = partition.use(dist.get_rank())\n        train_set = torch.utils.data.DataLoader(partition,\n                                             batch_size=bsz,\n                                             shuffle=True)\n        return train_set, bsz\n\nAssuming we have 2 replicas, then each process will have a ``train_set``\nof 60000 / 2 = 30000 samples. We also divide the batch size by the\nnumber of replicas in order to maintain the *overall* batch size of 128.\n\nWe can now write our usual forward-backward-optimize training code, and\nadd a function call to average the gradients of our models. (The\nfollowing is largely inspired from the official `PyTorch MNIST\nexample <https://github.com/pytorch/examples/blob/master/mnist/main.py>`__.)\n\n.. code:: python\n\n    \"\"\" Distributed Synchronous SGD Example \"\"\"\n    def run(rank, size):\n            torch.manual_seed(1234)\n            train_set, bsz = partition_dataset()\n            model = Net()\n            optimizer = optim.SGD(model.parameters(),\n                                  lr=0.01, momentum=0.5)\n\n            num_batches = ceil(len(train_set.dataset) / float(bsz)) \n            for epoch in range(10):\n                epoch_loss = 0.0\n                for data, target in train_set:\n                    data, target = Variable(data), Variable(target)\n                    optimizer.zero_grad()\n                    output = model(data)\n                    loss = F.nll_loss(output, target)\n                    epoch_loss += loss.data[0]\n                    loss.backward()\n                    average_gradients(model)\n                    optimizer.step()\n                print('Rank ', dist.get_rank(), ', epoch ',\n                      epoch, ': ', epoch_loss / num_batches) \n\nIt remains to implement the ``average_gradients(model)`` function, which\nsimply takes in a model and averages its gradients across the whole\nworld.\n\n.. code:: python\n\n    \"\"\" Gradient averaging. \"\"\"\n    def average_gradients(model):\n        size = float(dist.get_world_size())\n        for param in model.parameters():\n            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n            param.grad.data /= size \n\n*Et voil\u00e0 *! We successfully implemented distributed synchronous SGD and\ncould train any model on a large computer cluster.\n\n**Note:** While the last sentence is *technically* true, there are `a\nlot more tricks <http://seba-1511.github.io/dist_blog>`__ required to\nimplement a production-level implementation of synchronous SGD. Again,\nuse what `has been tested and\noptimized <http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel>`__.\n\nOur Own Ring-Allreduce\n----------------------\n\nAs an additional challenge, imagine that we wanted to implement\nDeepSpeech's efficient ring allreduce. This is fairly easily implemented\nusing point-to-point collectives.\n\n.. code:: python\n\n    \"\"\" Implementation of a ring-reduce with addition. \"\"\"\n    def allreduce(send, recv):\n        rank = dist.get_rank()\n        size = dist.get_world_size()\n        send_buff = th.zeros(send.size())\n        recv_buff = th.zeros(send.size())\n        accum = th.zeros(send.size())\n        accum[:] = send[:]\n\n        left = ((rank - 1) + size) % size\n        right = (rank + 1) % size\n\n        for i in range(size - 1):\n            if i % 2 == 0:\n                # Send send_buff\n                send_req = dist.isend(send_buff, right)\n                dist.recv(recv_buff, left)\n                accum[:] += recv[:]\n            else:\n                # Send recv_buff\n                send_req = dist.isend(recv_buff, right)\n                dist.recv(send_buff, left)\n                accum[:] += send[:]\n            send_req.wait()\n        recv[:] = accum[:]\n\nIn the above script, the ``allreduce(send, recv)`` function has a\nslightly different signature than the ones in PyTorch. It takes a\n``recv`` tensor and will store the sum of all ``send`` tensors in it. As\nan exercise left to the reader, there is still one difference between\nour version and the one in DeepSpeech: their implementation divide the\ngradient tensor into *chunks*, so as to optimially utilize the\ncommunication bandwidth. (Hint:\n`toch.chunk <http://pytorch.org/docs/master/torch.html#torch.chunk>`__)\n\nAdvanced Topics\n===============\n\nWe are now ready to discover some of the more advanced functionalities\nof ``torch.distributed``. Since there is a lot to cover, this section is\ndivided into two subsections:\n\n1. Communication Backends: where we learn how to use MPI and Gloo for\n   GPU-GPU communication.\n2. Initialization Methods: where we understand how to best setup the\n   initial coordination phase in ``dist.init_process_group()``.\n\nCommunication Backends\n----------------------\n\nOne of the most elegant aspects of ``torch.distributed`` is its ability\nto abstract and build on top of different backends. As mentioned before,\nthere are currently three backends implemented in PyTorch: TCP, MPI, and\nGloo. They each have different specifications and tradeoffs, depending\non the desired use-case. A comparative table of supported functions can\nbe found\n`here <http://pytorch.org/docs/master/distributed.html#module-torch.distributed>`__.\n\nTCP Backend\n~~~~~~~~~~~\n\nSo far we have made extensive usage of the TCP backend. It is quite\nhandy as a development platform, as it is guaranteed to work on most\nmachines and operating systems. It also supports all point-to-point and\ncollective functions on CPU. However, there is no support for GPUs and\nits communication routines are not as optimized as the MPI one.\n\nGloo Backend\n~~~~~~~~~~~~\n\nThe `Gloo backend <https://github.com/facebookincubator/gloo>`__\nprovides an optimized implementation of *collective* communication\nprocedures, both for CPUs and GPUs. It particularly shines on GPUs as it\ncan perform communication without transferring data to the CPU's memory\nusing `GPUDirect <https://developer.nvidia.com/gpudirect>`__. It is also\ncapable of using `NCCL <https://github.com/NVIDIA/nccl>`__ to perform\nfast intra-node communication and implements its `own\nalgorithms <https://github.com/facebookincubator/gloo/blob/master/docs/algorithms.md>`__\nfor inter-node routines.\n\nSince version 0.2.0, the Gloo backend is automatically included with the\npre-compiled binaries of PyTorch. As you have surely noticed, our\ndistributed SGD example does not work if you put ``model`` on the GPU.\nLet's fix it by first replacing ``backend='gloo'`` in\n``init_processes(rank, size, fn, backend='tcp')``. At this point, the\nscript will still run on CPU but uses the Gloo backend behind the\nscenes. In order to use multiple GPUs, let us also do the following\nmodifications:\n\n0. ``init_processes(rank, size, fn, backend='tcp')`` :math:`\\rightarrow`\n   ``init_processes(rank, size, fn, backend='gloo')``\n1. ``model = Net()`` :math:`\\rightarrow` ``model = Net().cuda(rank)``\n2. ``data, target = Variable(data), Variable(target)``\n   :math:`\\rightarrow`\n   ``data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))``\n\nWith the above modifications, our model is now training on two GPUs and\nyou can monitor their utilization with ``watch nvidia-smi``.\n\nMPI Backend\n~~~~~~~~~~~\n\nThe Message Passing Interface (MPI) is a standardized tool from the\nfield of high-performance computing. It allows to do point-to-point and\ncollective communications and was the main inspiration for the API of\n``torch.distributed``. Several implementations of MPI exist (e.g.\n`Open-MPI <https://www.open-mpi.org/>`__,\n`MVAPICH2 <http://mvapich.cse.ohio-state.edu/>`__, `Intel\nMPI <https://software.intel.com/en-us/intel-mpi-library>`__) each\noptimized for different purposes. The advantage of using the MPI backend\nlies in MPI's wide availability - and high-level of optimization - on\nlarge computer clusters. `Some <https://developer.nvidia.com/mvapich>`__\n`recent <https://developer.nvidia.com/ibm-spectrum-mpi>`__\n`implementations <http://www.open-mpi.org/>`__ are also able to take\nadvantage of CUDA IPC and GPU Direct technologies in order to avoid\nmemory copies through the CPU.\n\nUnfortunately, PyTorch's binaries can not include an MPI implementation\nand we'll have to recompile it by hand. Fortunately, this process is\nfairly simple given that upon compilation, PyTorch will look *by itself*\nfor an available MPI implementation. The following steps install the MPI\nbackend, by installing PyTorch `from\nsources <https://github.com/pytorch/pytorch#from-source>`__.\n\n1. Create and activate your Anaconda environment, install all the\n   pre-requisites following `the\n   guide <https://github.com/pytorch/pytorch#from-source>`__, but do\n   **not** run ``python setup.py install`` yet.\n2. Choose and install your favorite MPI implementation. Note that\n   enabling CUDA-aware MPI might require some additional steps. In our\n   case, we'll stick to Open-MPI *without* GPU support:\n   ``conda install -c conda-forge openmpi``\n3. Now, go to your cloned PyTorch repo and execute\n   ``python setup.py install``.\n\nIn order to test our newly installed backend, a few modifications are\nrequired.\n\n1. Replace the content under ``if __name__ == '__main__':`` with\n   ``init_processes(0, 0, run, backend='mpi')``.\n2. Run ``mpirun -n 4 python myscript.py``.\n\nThe reason for these changes is that MPI needs to create its own\nenvironment before spawning the processes. MPI will also spawn its own\nprocesses and perform the handshake described in `Initialization\nMethods <#initialization-methods>`__, making the ``rank``\\ and ``size``\narguments of ``init_process_group`` superfluous. This is actually quite\npowerful as you can pass additional arguments to ``mpirun`` in order to\ntailor computational resources for each process. (Things like number of\ncores per process, hand-assigning machines to specific ranks, and `some\nmore <https://www.open-mpi.org/faq/?category=running#mpirun-hostfile>`__)\nDoing so, you should obtain the same familiar output as with the other\ncommunication backends.\n\nInitialization Methods\n----------------------\n\nTo finish this tutorial, let's talk about the very first function we\ncalled: ``dist.init_process_group(backend, init_method)``. In\nparticular, we will go over the different initialization methods which\nare responsible for the initial coordination step between each process.\nThose methods allow you to define how this coordination is done.\nDepending on your hardware setup, one of these methods should be\nnaturally more suitable than the others. In addition to the following\nsections, you should also have a look at the `official\ndocumentation <http://pytorch.org/docs/master/distributed.html#initialization>`__.\n\nBefore diving into the initialization methods, let's have a quick look\nat what happens behind ``init_process_group`` from the C/C++\nperspective.\n\n1. First, the arguments are parsed and validated.\n2. The backend is resolved via the ``name2channel.at()`` function. A\n   ``Channel`` class is returned, and will be used to perform the data\n   transmission.\n3. The GIL is dropped, and ``THDProcessGroupInit()`` is called. This\n   instantiates the channel and adds the address of the master node.\n4. The process with rank 0 will execute the ``master`` procedure, while\n   all other ranks will be ``workers``.\n5. The master\n\n   a. Creates sockets for all workers.\n   b. Waits for all workers to connect.\n   c. Sends them information about the location of the other processes.\n\n6. Each worker\n\n   a. Creates a socket to the master.\n   b. Sends their own location information.\n   c. Receives information about the other workers.\n   d. Opens a socket and handshakes with all other workers.\n\n7. The initialization is done, and everyone is connected to everyone.\n\nEnvironment Variable\n~~~~~~~~~~~~~~~~~~~~\n\nWe have been using the environment variable initialization method\nthroughout this tutorial. By setting the following four environment\nvariables on all machines, all processes will be able to properly\nconnect to the master, obtain information about the other processes, and\nfinally handshake with them.\n\n-  ``MASTER_PORT``: A free port on the machine that will host the\n   process with rank 0.\n-  ``MASTER_ADDR``: IP address of the machine that will host the process\n   with rank 0.\n-  ``WORLD_SIZE``: The total number of processes, so that the master\n   knows how many workers to wait for.\n-  ``RANK``: Rank of each process, so they will know whether it is the\n   master of a worker.\n\nShared File System\n~~~~~~~~~~~~~~~~~~\n\nThe shared filesystem requires all processes to have access to a shared\nfile system, and will coordinate them through a shared file. This means\nthat each process will open the file, write its information, and wait\nuntil everybody did so. After what all required information will be\nreadily available to all processes. In order to avoid race conditions,\nthe file system must support locking through\n`fcntl <http://man7.org/linux/man-pages/man2/fcntl.2.html>`__. Note that\nyou can specify ranks manually or let the processes figure it out by\nthemselves. Be defining a unique ``groupname`` per job you can use the\nsame file path for multiple jobs and safely avoid collision.\n\n.. code:: python\n\n    dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4,\n                            group_name='mygroup')\n\nTCP Init & Multicast\n~~~~~~~~~~~~~~~~~~~~\n\nInitializing via TCP can be achieved in two different ways:\n\n1. By providing the IP address of the process with rank 0 and the world\n   size.\n2. By providing *any* valid IP `multicast\n   address <https://en.wikipedia.org/wiki/Multicast_address>`__ and the\n   world size.\n\nIn the first case, all workers will be able to connect to the process\nwith rank 0 and follow the procedure described above.\n\n.. code:: python\n\n    dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)\n\nIn the second case, the multicast address specifies the group of nodes\nwho might potentially be active and the coordination can be handled by\nallowing each process to have an initial handshake before following the\nabove procedure. In addition TCP multicast initialization also supports\na ``group_name`` argument (as with the shared file method) allowing\nmultiple jobs to be scheduled on the same cluster.\n\n.. code:: python\n\n    dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',\n                            world_size=4)\n\n.. raw:: html\n\n   <!--\n   ## Internals\n   * The magic behind init_process_group:\n\n   1. validate and parse the arguments\n   2. resolve the backend: name2channel.at()\n   3. Drop GIL & THDProcessGroupInit: instantiate the channel and add address of master from config\n   4. rank 0 inits master, others workers\n   5. master: create sockets for all workers -> wait for all workers to connect -> send them each the info about location of other processes\n   6. worker: create socket to master, send own info, receive info about each worker, and then handshake with each of them\n   7. By this time everyone has handshake with everyone.\n   -->\n\n.. raw:: html\n\n   <center>\n\n**Acknowledgements**\n\n.. raw:: html\n\n   </center>\n\nI'd like to thank the PyTorch developers for doing such a good job on\ntheir implementation, documentation, and tests. When the code was\nunclear, I could always count on the\n`docs <http://pytorch.org/docs/master/distributed.html>`__ or the\n`tests <https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py>`__\nto find an answer. In particular, I'd like to thank Soumith Chintala,\nAdam Paszke, and Natalia Gimelshein for providing insightful comments\nand answering questions on early drafts.\n\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}